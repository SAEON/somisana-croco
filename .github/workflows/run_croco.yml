name: run croco configuration

on:
  workflow_call:
    inputs:
      RUN_DATE:
        description: 'time of T0 of the model run in format YYYYMMDD_HH - defined dynamically in run_ops.yml'
        required: true
        type: string
      BRANCH_REF:
        description: 'what branch are we on - defined dynamically in run_ops.yml'
        required: true
        type: string
      RUNNER_NAME:
        description: 'specify the runner name to determine what server we are running on'
        required: true
        type: string
      MODEL:
        description: 'directory name used to define the model e.g.croco_v1.3.1'
        required: true
        type: string
      DOMAIN:
        description: 'directory name used to define the domain e.g. sa_west_02' 
        required: true
        type: string
      COMP:
        description: 'directory corresponding to the CROCO compilation options e.g. C01' 
        required: true
        type: string
      INP:
        description: 'directory corresponding to the CROCO run-time input options e.g. I01' 
        required: true
        type: string
      BLK:
        description: 'name of bulk forcing e.g. GFS' 
        required: true
        type: string
      FRC:
        description: 'name of forcing file (placeholder for now - I think we need it when we start including tides)' 
        required: true
        type: string
      OGCM:
        description: 'name of boundary forcing e.g. MERCATOR' 
        required: true
        type: string
      HDAYS:
        description: 'number of hindcast days (integer) from T0'
        required: true
        type: string
      FDAYS:
        description: 'number of forecast days (integer) from T0'
        required: true
        type: string

env:
  # define the directory names for this configuration
  # unfortunately it looks like you can't use vars defined here to define others i.e. you can't use {{ env.* }} inside this block, you have to use {{ inputs.* }} for each var
  BRANCH_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}
  DATE_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}/${{ inputs.RUN_DATE }}
  DATA_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}/${{ inputs.RUN_DATE }}/downloaded_data
  CONFIG_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}/${{ inputs.RUN_DATE }}/${{ inputs.DOMAIN }}/${{ inputs.MODEL }}
  RUN_NAME: ${{ inputs.COMP }}_${{ inputs.INP }}_${{ inputs.OGCM }}_${{ inputs.BLK }}

jobs:
  setup_rundir:
    runs-on: ${{ inputs.RUNNER_NAME }}
    outputs:
      RUN_DIR: ${{ steps.make_dir.outputs.run_dir }}
      SCRATCH_DIR: ${{ steps.make_dir.outputs.scratch_dir }}
    continue-on-error: true
    steps:
      - name: create the run directory
        id: make_dir
        run: |
          rm -rf ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}  
          mkdir -p ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/{scratch,output,postprocess}
          chown -R :runners ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}
          chmod -R 774 ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}
          # output dirs for use in other jobs
          echo "run_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}" >> $GITHUB_OUTPUT 
          echo "scratch_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/scratch" >> $GITHUB_OUTPUT 
          echo "output_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/output" >> $GITHUB_OUTPUT 
          echo "postprocess_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/postprocess" >> $GITHUB_OUTPUT 
  
  # see if we can restart from a previous run
  get_rst:
    needs: [setup_rundir]
    runs-on: ${{ inputs.RUNNER_NAME }}
    env:
      SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
    outputs:
      MAKE_INI: ${{ steps.check_restart.outputs.make_ini }}
      RST_DATE: ${{ steps.check_restart.outputs.rst_date }}
      RST_STEP: ${{ steps.check_restart.outputs.rst_step }}
    steps:
      - name: Try to find a suitable restart file
        id: check_restart
        run: |
          # get the run_date as an epoch time (seconds since 1970-01-01)
          run_date=${{ inputs.RUN_DATE }}
          run_date_epoch_time=$(date -d "${run_date:0:4}-${run_date:4:2}-${run_date:6:2} ${run_date:9:2}:00:00" +%s)
          # current_date starts at run_date and gets changed iteratively in the while loop below
          current_date=${{ inputs.RUN_DATE }} 
          step_count=0
          MAX_STEPS=8 # number of 6 hour steps back to look for a restart file - preferred to restarting from global data  

          while [[ $step_count -lt $MAX_STEPS ]]; do

            # Calculate previous date (6 hours back)
            # Convert the datetime to epoch time (seconds since 1970-01-01)
            epoch_time=$(date -d "${current_date:0:4}-${current_date:4:2}-${current_date:6:2} ${current_date:9:2}:00:00" +%s)
            # Subtract 6 hours in seconds (6 hours * 60 minutes * 60 seconds)
            current_date_epoch_time=$((epoch_time - 6 * 60 * 60))
            # update $current_date
            current_date=$(date -d "@$current_date_epoch_time" +%Y%m%d_%H)

            # restart file to look for
            rst_file="${{ env.BRANCH_DIR }}/${current_date}/${{ inputs.DOMAIN }}/${{ inputs.MODEL }}/${{ env.RUN_NAME }}/output/croco_rst.nc"

            # Check if the file exists
            if [[ -f "$rst_file" ]]; then
              echo "restart file found at: $rst_file"
              echo "copy to ${{ env.SCRATCH_DIR }}/croco_ini.nc"
              cp $rst_file ${{ env.SCRATCH_DIR }}/croco_ini.nc
              # compute the number of 6 hourly intervals between run_date and current_date - 
              # this is needed to edit the .in file so we initialise from the correct time-step in the restart file
              # (I guess I could have rather used $step_count + 1 here... should be same difference)
              rst_step=$(((run_date_epoch_time - current_date_epoch_time) / (3600*6)))

              # Exit the loop since the restart file is found
              break
            else
              echo "restart file not found: $rst_file"
              step_count=$((step_count + 1))
            fi
          done

          if [[ $step_count -eq $MAX_STEPS ]]; then
            echo "using ${{ inputs.OGCM }} to interpolate initial conditions"
            echo "make_ini=1" >> $GITHUB_OUTPUT
            echo "rst_date=NA" >> $GITHUB_OUTPUT
            # since we're using an ini file, set rst_step to 1, which means we'll use the first value in the ini file 
            echo "rst_step=1" >> $GITHUB_OUTPUT
          else
            echo "make_ini=0" >> $GITHUB_OUTPUT
            echo "rst_date=$current_date" >> $GITHUB_OUTPUT
            echo "rst_step=$rst_step" >> $GITHUB_OUTPUT
          fi
  
  # make the boundary forcing          
  get_ogcm:
    needs: [setup_rundir,get_rst]
    runs-on: ${{ inputs.RUNNER_NAME }}
    env:
      SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
      make_ini: ${{ needs.get_rst.outputs.MAKE_INI}} 
    steps:
      - name: make temporary directory for the reformatted OGCM file
        # fyi this is not needed for the blk files as the reformated blk file is saved in the directory where the data are downloaded
        run: |
          rm -rf ${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/tmp_for_croco
          mkdir -p ${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/tmp_for_croco
          chown -R :runners ${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/tmp_for_croco
          chmod -R 774 ${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/tmp_for_croco

      # consider writing a *.env file as part of the OGCM download process so we don't have to pass make_${OGCM}_ocims.m any input arguments, like it's done with GFS?
      - name: Format RUN_DATE
        id: format_date
        run: |
          run_date=${{ inputs.RUN_DATE }}
          run_date_formatted="'${run_date:0:4}-${run_date:4:2}-${run_date:6:2} ${run_date:9:2}'"
          echo "value=$run_date_formatted" >> $GITHUB_OUTPUT
      - name: generate the OGCM forcing file (and ini file if restart file is not available)
        id: make_ogcm_file
        run: >-
          docker run \
            --rm \
            --mac-address 02:42:ff:ff:ff:ff \
            --entrypoint /bin/bash \
            -v /opt/licenses/matlab-r2022a/license.lic:/licenses/license.lic \
            -v ${{ env.DATE_DIR }}:/home/matlab/somisana \
            -e MLM_LICENSE_FILE=/licenses/license.lic \
            ghcr.io/saeon/somisana-croco_matlab_${{ inputs.BRANCH_REF }}:latest \
              -c "cd /home/matlab/somisana/${{ inputs.DOMAIN }}/${{ inputs.MODEL }}/${{ inputs.OGCM }} && matlab -nodisplay -nosplash -nodesktop -r \"start; make_${{ inputs.OGCM }}_ocims(${{ steps.format_date.outputs.value }},${{ inputs.HDAYS }},${{ env.make_ini }}); exit;\""
      - name: copy OGCM file to scratch dir
        run: |
          ogcm_file="${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/croco_clm_${{ inputs.OGCM }}_${{ inputs.RUN_DATE }}.nc"
          cp ${ogcm_file} ${{ env.SCRATCH_DIR }}/croco_clm.nc
      - name: copy ini file to scratch dir, if made from OGCM
        run: |
          make_ini=${{ env.make_ini }}
          if [[ ${make_ini} == 1 ]]; then
            ini_file="${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/croco_ini_${{ inputs.OGCM }}_${{ inputs.RUN_DATE }}.nc"
            cp ${ini_file} ${{ env.SCRATCH_DIR }}/croco_ini.nc
          fi

  # make the surface forcing
  # (not needed anymore since we are using the ONLINE cpp key)
  #  get_blk:
  #  needs: [setup_rundir]
  #  env:
  #    SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
  #  runs-on: ${{ inputs.RUNNER_NAME }}
  #  steps:
  #    - name: generate the BLK forcing file
  #      id: make_blk_file
  #      run: >-
  #        docker run \
  #          --rm \
  #          --mac-address 02:42:ff:ff:ff:ff \
  #          --entrypoint /bin/bash \
  #          -v /opt/licenses/matlab-r2022a/license.lic:/licenses/license.lic \
  #          -v ${{ env.DATE_DIR }}:/home/matlab/somisana \
  #          -e MLM_LICENSE_FILE=/licenses/license.lic \
  #          ghcr.io/saeon/somisana-croco_matlab_${{ inputs.BRANCH_REF }}:latest \
  #            -c "cd /home/matlab/somisana/${{ inputs.DOMAIN }}/${{ inputs.MODEL }}/${{ inputs.BLK }} && matlab -nodisplay -nosplash -nodesktop -r \"start; make_${{ inputs.BLK }}_ocims; exit;\""
  #    - name: copy BLK file to scratch dir
  #      run: |
  #        blk_file="${{ env.CONFIG_DIR }}/${{ inputs.BLK }}/croco_blk_${{ inputs.BLK }}_${{ inputs.RUN_DATE }}.nc"
  #        cp ${blk_file} ${{ env.SCRATCH_DIR }}/croco_blk.nc
  
  # get the runtime input file        
  get_inp:
    needs: [setup_rundir,get_rst]
    env:
      SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
      RST_STEP: ${{ needs.get_rst.outputs.RST_STEP}} 
    runs-on: ${{ inputs.RUNNER_NAME }}
    steps:
      - name: prepare the runtime input file
        run: |
          # compute the time parameters, using the my_env_in.sh file for this configuration
          source ${{ env.CONFIG_DIR }}/${{ inputs.INP }}/myenv_in.sh
          HDAYS=${{ inputs.HDAYS }}
          FDAYS=${{ inputs.FDAYS }}
          NDAYS=$((HDAYS + FDAYS))
          NUMTIMES=$((NDAYS * 24 * 3600 / DT))
          NUMAVG=$((NH_AVG * 3600 / DT))
          NUMHIS=$((NH_HIS * 3600 / DT))
          # NUMSTA=$((NH_STA * 3600 / DT)) - station output not getting used as we can easily extract time-series as a postprocessing step
          NUMAVGSURF=$((NH_AVGSURF * 3600 / DT))
          NUMHISSURF=$((NH_HISSURF * 3600 / DT))
          NUMRST=$((NH_RST * 3600 / DT)) # frequency of output in restart file to be written
          RST_STEP=${{ env.RST_STEP }} # which time-step to restart from in this run
          # directory where the surface files are for ONLINE interpolation
          DATA_DIR_BLK=${{ env.DATA_DIR }}/${{ inputs.BLK }}/for_croco/

          # do a sed replacement on the template .in file, and write the output to the scratch dir
          sed -e 's|DTNUM|'$DT'|' \
            -e 's|DTFAST|'$DTFAST'|' \
            -e 's|NUMTIMES|'$NUMTIMES'|' \
            -e 's|NUMHISSURF|'$NUMHISSURF'|' \
            -e 's|NUMAVGSURF|'$NUMAVGSURF'|' \
            -e 's|NUMHIS|'$NUMHIS'|' \
            -e 's|NUMAVG|'$NUMAVG'|' \
            -e 's|RST_STEP|'$RST_STEP'|' \
            -e 's|NUMRST|'$NUMRST'|' \
            -e 's|DATA_DIR|'${DATA_DIR_BLK}'|' \
            < ${{ env.CONFIG_DIR }}/${{ inputs.INP }}/croco_fcst.in \
            > ${{ env.SCRATCH_DIR }}/croco.in

  # compile the code
  compile_croco:
    needs: [setup_rundir]
    env:
      SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
    runs-on: ${{ inputs.RUNNER_NAME }}
    steps:
      - name: compile the code
        run: |
          docker run \
            --rm \
            --entrypoint /bin/bash \
            -v ${{ env.CONFIG_DIR }}:/home/somisana \
            ghcr.io/saeon/somisana-croco_run_main:latest \
              -c "cd /home/somisana && ./jobcomp_frcst.sh ${{ inputs.COMP }}"
      - name: copy executable to scratch dir
        run: |
          cp ${{ env.CONFIG_DIR }}/${{ inputs.COMP }}/croco ${{ env.SCRATCH_DIR }}

  # run the model
  run_croco:
    needs: [setup_rundir,compile_croco,get_inp,get_ogcm,get_rst]
    env:
      SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
    runs-on: ${{ inputs.RUNNER_NAME }}
    steps:
      - name: get the grid file (the only outstanding input)
        run: |
          grd_file="${{ env.CONFIG_DIR }}/GRID/croco_grd.nc"
          cp ${grd_file} ${{ env.SCRATCH_DIR }}/croco_grd.nc
      - name: run the model
        run: |
          docker run \
            --rm \
            --entrypoint /bin/bash \
            -v ${{ env.CONFIG_DIR }}:/home/somisana \
            ghcr.io/saeon/somisana-croco_run_main:latest \
              -c "cd /home/somisana && ./run_croco_frcst.bash ${{ env.RUN_NAME }}"

  # postprocess the raw croco output files into something more user-friendly
  postprocess:
    needs: [setup_rundir,run_croco]
    env:
      RUN_DIR: ${{ needs.setup_rundir.outputs.RUN_DIR}}
    runs-on: ${{ inputs.RUNNER_NAME }}
    steps:
      - name: subset the raw CROCO NetCDF file for archiving
        continue-on-error: true
        run: |
          # we only want to archive the nowcast to forecast, so we chop off the hindcast part of the output
          # this is dependent on the avg output frequency, which we read from the myenv_in.sh file
          source ${{ env.CONFIG_DIR }}/${{ inputs.INP }}/myenv_in.sh
          HINDCAST_DAYS=${{ inputs.HDAYS }}
          HINDCAST_HOURS=$((HINDCAST_DAYS * 24))
          HINDCAST_NUMAVG=$((HINDCAST_HOURS / NH_AVG)) # NH_AVG is read from myenv_in.sh - it's the output frequency (in hours) of the avg file
          #
          docker run \
            --rm \
            --entrypoint /bin/bash \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_run_main:latest \
            -c "ncks -v temp,salt,u,v,w,zeta,sustr,svstr -d time,${HINDCAST_NUMAVG}, -O /tmp/output/croco_avg.nc /tmp/postprocess/croco_avg_frcst.nc"
      # Regrid CROCO u,v to rho grid,
      # rotate u,v components from grid aligned to east/north aligned and
      # work out depth levels of sigma grid in meters (tier 1)
      - name: Tier 1 regridding
        continue-on-error: true
        run: |
          docker run \
            --user $(id -u):$(id -g) \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              regrid_tier1 \
                --fname /tmp/output/croco_avg.nc \
                --fname_out /tmp/postprocess/croco_avg_tier1.nc \
                --ref_date '2000-01-01 00:00:00'
      # interpolate data to constant vertical levels (tier 2)
      - name: Tier 2 regridding
        continue-on-error: true
        run: |
          docker run \
            --user $(id -u):$(id -g) \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              regrid_tier2 \
                --fname /tmp/postprocess/croco_avg_tier1.nc \
                --fname_out /tmp/postprocess/croco_avg_tier2.nc \
                --depths 0,-5,-10,-50,-100,-500,-1000,-99999
      # and interpolate data to a constant horizontal grid (tier 3)
      - name: Tier 3 regridding
        continue-on-error: true
        run: |
          docker run \
            --user $(id -u):$(id -g) \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              regrid_tier3 \
                --fname /tmp/postprocess/croco_avg_tier2.nc \
                --fname_out /tmp/postprocess/croco_avg_tier3.nc \
                --spacing 0.02
       # here we can add the extraction of time-series 
       # for this there needs to be an ascii file for each config_dir with the coordinates and names of the time-series locations

  # copy data over to where it will be archived
  archive:
    needs: [setup_rundir,postprocess]
    runs-on: ${{ inputs.RUNNER_NAME }}
    env:
      RUN_DIR: ${{ needs.setup_rundir.outputs.RUN_DIR}}
    steps:
      - name: get archive dir
        id: archive_dir
        run: |
          run_date=${{ inputs.RUN_DATE }}
          run_date_yyyymm=${run_date:0:6}
          # cut off the last three characters of the domain name e.g. the *_02 part
          domain=$(echo ${{ inputs.DOMAIN }} | sed 's/...$//')
          archive_dir=/mnt/saeon-somisana/data/${domain}/v1.0/forecasts/${run_date_yyyymm}/${run_date}/${{ inputs.OGCM }}-${{ inputs.BLK }}
          echo "value=$archive_dir" >> $GITHUB_OUTPUT
      - name: create the archive directory if needed
        run: |
          if [ ! -d ${{ steps.archive_dir.outputs.value }} ]; then
            sudo mkdir -p ${{ steps.archive_dir.outputs.value }}
            sudo chmod -R 774 ${{ steps.archive_dir.outputs.value }}
          fi
      - name: copy files to the archive directory if needed
        run: |
          cp ${{ env.RUN_DIR }}/postprocess/croco_avg_tier1.nc ${{ steps.archive_dir.outputs.value }}
          cp ${{ env.RUN_DIR }}/postprocess/croco_avg_tier2.nc ${{ steps.archive_dir.outputs.value }}
          cp ${{ env.RUN_DIR }}/postprocess/croco_avg_tier3.nc ${{ steps.archive_dir.outputs.value }}
          cp ${{ env.RUN_DIR }}/postprocess/croco_avg.nc ${{ steps.archive_dir.outputs.value }}
          cp ${{ env.RUN_DIR }}/postprocess/croco_avg_frcst.nc ${{ steps.archive_dir.outputs.value }}
 
