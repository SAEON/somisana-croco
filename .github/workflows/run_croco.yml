name: run croco configuration

on:
  workflow_call:
    inputs:
      RUN_DATE:
        description: 'time of T0 of the model run in format YYYYMMDD_HH - defined dynamically in run_ops.yml'
        required: true
        type: string
      BRANCH_REF:
        description: 'what branch are we on - defined dynamically in run_ops.yml'
        required: true
        type: string
      RUNNER_NAME:
        description: 'specify the runner name to determine what server we are running on'
        required: true
        type: string
      MODEL:
        description: 'directory name used to define the model e.g.croco_v1.3.1'
        required: true
        type: string
      DOMAIN:
        description: 'directory name used to define the domain e.g. sa_west_02' 
        required: true
        type: string
      VERSION:
        description: 'the version used in the archiving directory e.g. v1.0 (this is intended to handle major changes to the model in the forecasts)' 
        required: true
        type: string
      COMP:
        description: 'directory corresponding to the CROCO compilation options e.g. C01' 
        required: true
        type: string
      INP:
        description: 'directory corresponding to the CROCO run-time input options e.g. I01' 
        required: true
        type: string
      BLK:
        description: 'name of bulk forcing e.g. GFS' 
        required: true
        type: string
      FRC:
        description: 'name of forcing file (placeholder for now - I think we need it when we start including tides)' 
        required: true
        type: string
      OGCM:
        description: 'name of boundary forcing e.g. MERCATOR' 
        required: true
        type: string
      HDAYS:
        description: 'number of hindcast days (integer) from T0'
        required: true
        type: string
      FDAYS:
        description: 'number of forecast days (integer) from T0'
        required: true
        type: string

env:
  # define the directory names for this configuration
  # unfortunately it looks like you can't use vars defined here to define others i.e. you can't use {{ env.* }} inside this block, you have to use {{ inputs.* }} for each var
  BRANCH_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}
  DATE_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}/${{ inputs.RUN_DATE }}
  DATA_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}/${{ inputs.RUN_DATE }}/downloaded_data
  CONFIG_DIR: /home/somisana/ops/${{ inputs.BRANCH_REF }}/${{ inputs.RUN_DATE }}/${{ inputs.DOMAIN }}/${{ inputs.MODEL }}
  RUN_NAME: ${{ inputs.COMP }}_${{ inputs.INP }}_${{ inputs.OGCM }}_${{ inputs.BLK }}

jobs:
  setup_rundir:
    runs-on: ${{ inputs.RUNNER_NAME }}
    outputs:
      RUN_DIR: ${{ steps.make_dir.outputs.run_dir }}
      SCRATCH_DIR: ${{ steps.make_dir.outputs.scratch_dir }}
    continue-on-error: true
    steps:
      - name: create the run directory
        id: make_dir
        run: |
          sudo rm -rf ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}  
          mkdir -p ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/{scratch,output,postprocess}
          chown -R :runners ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}
          chmod -R 775 ${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}
          # output dirs for use in other jobs
          echo "run_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}" >> $GITHUB_OUTPUT 
          echo "scratch_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/scratch" >> $GITHUB_OUTPUT 
          echo "output_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/output" >> $GITHUB_OUTPUT 
          echo "postprocess_dir=${{ env.CONFIG_DIR }}/${{ env.RUN_NAME }}/postprocess" >> $GITHUB_OUTPUT 
  
  get_input_files:
    needs: [setup_rundir]
    runs-on: ${{ inputs.RUNNER_NAME }}
    env:
      SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
    steps:
      - name: Try to find a suitable restart/ini file
        id: get_ini
        run: |
          # get the run_date as an epoch time (seconds since 1970-01-01)
          run_date=${{ inputs.RUN_DATE }}
          run_date_epoch_time=$(date -d "${run_date:0:4}-${run_date:4:2}-${run_date:6:2} ${run_date:9:2}:00:00" +%s)
          # current_date starts at run_date and gets changed iteratively in the while loop below
          current_date=${{ inputs.RUN_DATE }} 
          step_count=0
          # MAX_STEPS defines the number of 6 hour steps back to look for a restart file 
          # It's not obvious to me what this should be. Of course we prefer to restart from our model rather than the global model
          # But the further go back to look for a restart file, the more of a forecast solution you're using
          # So it's a trade-off between the most recent global solution vs an older high res solution
          MAX_STEPS=12   

          while [[ $step_count -lt $MAX_STEPS ]]; do

            # Calculate previous date (6 hours back)
            # Convert the datetime to epoch time (seconds since 1970-01-01)
            epoch_time=$(date -d "${current_date:0:4}-${current_date:4:2}-${current_date:6:2} ${current_date:9:2}:00:00" +%s)
            # Subtract 6 hours in seconds (6 hours * 60 minutes * 60 seconds)
            current_date_epoch_time=$((epoch_time - 6 * 60 * 60))
            # update $current_date
            current_date=$(date -d "@$current_date_epoch_time" +%Y%m%d_%H)

            # restart file to look for
            rst_file="${{ env.BRANCH_DIR }}/${current_date}/${{ inputs.DOMAIN }}/${{ inputs.MODEL }}/${{ env.RUN_NAME }}/output/croco_rst.nc"

            # Check if the file exists
            if [[ -f "$rst_file" ]]; then
              echo "restart file found at: $rst_file"
              echo "copy to ${{ env.SCRATCH_DIR }}/croco_ini.nc"
              cp $rst_file ${{ env.SCRATCH_DIR }}/croco_ini.nc
              # compute the number of 6 hourly intervals between run_date and current_date - 
              # this is needed to edit the .in file so we initialise from the correct time-step in the restart file
              # (I guess I could have rather used $step_count + 1 here... should be same difference)
              rst_step=$(((run_date_epoch_time - current_date_epoch_time) / (3600*6)))

              # Exit the loop since the restart file is found
              break
            else
              echo "restart file not found: $rst_file"
              step_count=$((step_count + 1))
            fi
          done

          if [[ $step_count -eq $MAX_STEPS ]]; then
            # use the ini file interpolated from the OGCM data
            echo "using ${{ inputs.OGCM }} to interpolate initial conditions"
            ini_file="${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/croco_ini_${{ inputs.OGCM }}_${{ inputs.RUN_DATE }}.nc"
            cp ${ini_file} ${{ env.SCRATCH_DIR }}/croco_ini.nc
            # since we're using an ini file from the OGCM data, set rst_step to 1 i.e. the first (and only) value in the ini file 
            echo "rst_step=1" >> $GITHUB_OUTPUT
          else
            echo "rst_step=$rst_step" >> $GITHUB_OUTPUT
          fi
      
      - name: copy OGCM boundary file to scratch dir
        id: get_bry
        run: |
          ogcm_file="${{ env.CONFIG_DIR }}/${{ inputs.OGCM }}/croco_bry_${{ inputs.OGCM }}_${{ inputs.RUN_DATE }}.nc"
          cp ${ogcm_file} ${{ env.SCRATCH_DIR }}/croco_bry.nc
      
      - name: copy BLK files to scratch dir
        id: get_blk
        # even though we are using ONLINE interpolation, let's copy the BLK data to this model run scratch dir 
        # so there are no issues with multiple parallel runs trying to access the same file
        run: |
          cp ${{ env.DATA_DIR }}/${{ inputs.BLK }}/for_croco/* ${{ env.SCRATCH_DIR }}
  
      - name: prepare the runtime input file
        id: get_inp
        run: |
          # compute the time parameters, using the my_env_in.sh file for this configuration
          source ${{ env.CONFIG_DIR }}/${{ inputs.INP }}/myenv_in.sh
          HDAYS=${{ inputs.HDAYS }}
          FDAYS=${{ inputs.FDAYS }}
          # NDAYS=$((HDAYS + FDAYS))
          # the use of 'awk' below allows for HDAYS or FDAYS to include decimal points
          NDAYS=$(awk "BEGIN {print $HDAYS + $FDAYS}")
          NUMTIMES=$(awk "BEGIN {print int($NDAYS * 24 * 3600 / $DT)}")
          NUMAVG=$((NH_AVG * 3600 / DT))
          NUMHIS=$((NH_HIS * 3600 / DT))
          # NUMSTA=$((NH_STA * 3600 / DT)) - station output not getting used as we can easily extract time-series as a postprocessing step
          NUMAVGSURF=$((NH_AVGSURF * 3600 / DT))
          NUMHISSURF=$((NH_HISSURF * 3600 / DT))
          NUMRST=$((NH_RST * 3600 / DT)) # frequency of output in restart file to be written
          RST_STEP=${{ steps.get_ini.outputs.rst_step }} # which time-step to restart from in this run
          # directory where the surface files are for ONLINE interpolation 
          # (this is the path inside the docker image which runs the model!)
          DATA_DIR_BLK=/home/somisana/${{ env.RUN_NAME }}/scratch/
          # (this is what the path would have been if we weren't running the model inside a docker image)
          # DATA_DIR_BLK=${{ env.DATA_DIR }}/${{ inputs.BLK }}/for_croco/
          #
          # do a sed replacement on the template .in file, and write the output to the scratch dir
          sed -e 's|DTNUM|'$DT'|' \
            -e 's|DTFAST|'$DTFAST'|' \
            -e 's|NUMTIMES|'$NUMTIMES'|' \
            -e 's|NUMHISSURF|'$NUMHISSURF'|' \
            -e 's|NUMAVGSURF|'$NUMAVGSURF'|' \
            -e 's|NUMHIS|'$NUMHIS'|' \
            -e 's|NUMAVG|'$NUMAVG'|' \
            -e 's|RST_STEP|'$RST_STEP'|' \
            -e 's|NUMRST|'$NUMRST'|' \
            -e 's|DATA_DIR|'${DATA_DIR_BLK}'|' \
            < ${{ env.CONFIG_DIR }}/${{ inputs.INP }}/croco_fcst.in \
            > ${{ env.SCRATCH_DIR }}/croco.in

      - name: get the grid file
        id: get_grd
        run: |
          cp ${{ env.CONFIG_DIR }}/GRID/croco_grd.nc ${{ env.SCRATCH_DIR }}

      - name: get the croco executable file
        id: get_exe
        run: |
          cp ${{ env.CONFIG_DIR }}/${{ inputs.COMP }}/croco ${{ env.SCRATCH_DIR }}

  # run the model
  run_croco:
    needs: [setup_rundir,get_input_files]
    env:
      RUN_DIR: ${{ needs.setup_rundir.outputs.RUN_DIR}}
      SCRATCH_DIR: ${{ needs.setup_rundir.outputs.SCRATCH_DIR}}
    runs-on: ${{ inputs.RUNNER_NAME }}
    steps:
      - name: run the model
        # this step sometimes gets cancelled by github while running
        # not sure why, but may have to do with intermittent connectivity?
        # the problem is often resolved by simply re-running the model manually
        # so I'm using this retry approach to do this automatically of it fails
        uses: nick-fields/retry@master
        with:
          timeout_minutes: 60 # Script is considered failed if this limit is reached
          retry_wait_seconds: 300 # Wait 5 minutes and try again
          max_attempts: 3
          retry_on: any
          warning_on_retry: true
          shell: bash
          continue_on_error: false
          on_retry_command: sudo rm -f ${{ env.SCRATCH_DIR }}/croco_avg*nc* && sudo rm -f ${{ env.SCRATCH_DIR }}/croco_his*nc* && sudo rm -f ${{ env.SCRATCH_DIR }}/croco_rst*nc* 
          command: |
            # get ${MPI_NUM_PROCS} i.e. the number of cpus to assign to the container
            # (this is an attempt to avoid the container from overloading the server, which has been an issue)
            source ${{ env.CONFIG_DIR }}/myenv_frcst.sh
            # run the model
            docker run \
              --rm \
              --entrypoint /bin/bash \
              --cpus=${MPI_NUM_PROCS} \
              -v ${{ env.CONFIG_DIR }}:/home/somisana \
              ghcr.io/saeon/somisana-croco_run_main:latest \
                -c "cd /home/somisana && ./run_croco_frcst.bash ${{ env.RUN_NAME }}"
      - name: ensure permissions
        run: |
          chmod -R 775 ${{ env.RUN_DIR }}

  # postprocess the raw croco output files into something more user-friendly
  postprocess:
    needs: [setup_rundir,run_croco]
    env:
      RUN_DIR: ${{ needs.setup_rundir.outputs.RUN_DIR}}
    runs-on: ${{ inputs.RUNNER_NAME }}
    steps:
      - name: subset the raw CROCO NetCDF file for archiving
        continue-on-error: true
        run: |
          # we only want to archive the nowcast to forecast, so we chop off the hindcast part of the output
          # this is dependent on the avg output frequency, which we read from the myenv_in.sh file
          source ${{ env.CONFIG_DIR }}/${{ inputs.INP }}/myenv_in.sh
          HINDCAST_DAYS=${{ inputs.HDAYS }}
          HINDCAST_HOURS=$((HINDCAST_DAYS * 24))
          HINDCAST_NUMAVG=$((HINDCAST_HOURS / NH_AVG)) # NH_AVG is read from myenv_in.sh - it's the output frequency (in hours) of the avg file
          #
          docker run \
            --rm \
            --entrypoint /bin/bash \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_run_main:latest \
            -c "ncks -v temp,salt,u,v,w,zeta,sustr,svstr -d time,${HINDCAST_NUMAVG}, -O /tmp/output/croco_avg.nc /tmp/postprocess/croco_avg_frcst.nc"
      # Regrid CROCO u,v to rho grid,
      # rotate u,v components from grid aligned to east/north aligned and
      # work out depth levels of sigma grid in meters (tier 1)
      - name: Tier 1 regridding
        continue-on-error: true
        run: |
          docker run \
            --user $(id -u):$(id -g) \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              regrid_tier1 \
                --fname /tmp/output/croco_avg.nc \
                --fname_out /tmp/postprocess/croco_avg_t1.nc \
                --ref_date '2000-01-01 00:00:00'
      # interpolate data to constant vertical levels (tier 2)
      - name: Tier 2 regridding
        continue-on-error: true
        run: |
          docker run \
            --user $(id -u):$(id -g) \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              regrid_tier2 \
                --fname /tmp/postprocess/croco_avg_t1.nc \
                --fname_out /tmp/postprocess/croco_avg_t2.nc \
                --depths 0,-5,-10,-50,-100,-500,-1000,-99999
      # and interpolate data to a constant horizontal grid (tier 3)
      - name: Tier 3 regridding
        continue-on-error: true
        run: |
          docker run \
            --user $(id -u):$(id -g) \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              regrid_tier3 \
                --fname /tmp/postprocess/croco_avg_t2.nc \
                --fname_out /tmp/postprocess/croco_avg_t3.nc \
                --spacing 0.02
        
      # here we can add the extraction of time-series 
      # for this there needs to be an ascii file for each config_dir with the coordinates and names of the time-series locations

  # make plots/animations of the outout
  plotting:
    needs: [setup_rundir,run_croco]
    env:
      RUN_DIR: ${{ needs.setup_rundir.outputs.RUN_DIR}}
    runs-on: ${{ inputs.RUNNER_NAME }}
    steps:
      - name: animate surface temperature and currents
        continue-on-error: true
        run: |
          docker run \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              crocplot \
                --fname /tmp/output/croco_avg.nc \
                --gif_out /tmp/postprocess/croco_avg_temp_surf.gif \
                --ref_date '2000-01-01 00:00:00'
          sudo chown somisana:somisana ${{ env.RUN_DIR }}/postprocess/croco_avg_temp_surf.gif
      - name: animate 100m temperature and currents
        continue-on-error: true
        run: |
          docker run \
            --rm \
            -v ${{ env.RUN_DIR }}:/tmp \
            ghcr.io/saeon/somisana-croco_cli_${{ inputs.BRANCH_REF }}:latest \
              crocplot \
                --fname /tmp/output/croco_avg.nc \
                --level -100 \
                --gif_out /tmp/postprocess/croco_avg_temp_100m.gif \
                --ref_date '2000-01-01 00:00:00'
          sudo chown somisana:somisana ${{ env.RUN_DIR }}/postprocess/croco_avg_temp_100m.gif
      - name: ensure permissions
        run: |
          chmod -R 775 ${{ env.RUN_DIR }}

  # copy data over to where it will be archived
  archive:
    needs: [setup_rundir,postprocess,plotting]
    runs-on: ${{ inputs.RUNNER_NAME }}
    env:
      RUN_DIR: ${{ needs.setup_rundir.outputs.RUN_DIR}}
    steps:
      - name: get archive dir
        id: archive_dir
        run: |
          run_date=${{ inputs.RUN_DATE }}
          run_date_yyyymm=${run_date:0:6}
          # cut off the last three characters of the domain name for the archive directory e.g. the *_02 part
          domain=$(echo ${{ inputs.DOMAIN }} | sed 's/...$//')
          # replace underscores with hyphens for the archive directory - probably looks better
          domain=${domain//_/-}
          #
          archive_dir=/mnt/ocims-somisana/${domain}/${{ inputs.VERSION }}/forecasts/${run_date_yyyymm}/${run_date}/${{ inputs.OGCM }}-${{ inputs.BLK }}
          echo "value=$archive_dir" >> $GITHUB_OUTPUT
      - name: copy files to the archive directory if needed
        run: |
          # only copy if we're on the main branch
          if [ ${{ inputs.BRANCH_REF }} = "main" ]; then
            if [ ! -d ${{ steps.archive_dir.outputs.value }} ]; then
              sudo mkdir -p ${{ steps.archive_dir.outputs.value }}
              sudo chmod -R 775 ${{ steps.archive_dir.outputs.value }}
            fi
            # apparently we need sudo permissions to copy to the archive directory
            # we have set up the somisana user to be able to do this without the need for a password
            sudo cp ${{ env.RUN_DIR }}/postprocess/croco_avg_t1.nc ${{ steps.archive_dir.outputs.value }}
            sudo cp ${{ env.RUN_DIR }}/postprocess/croco_avg_t2.nc ${{ steps.archive_dir.outputs.value }}
            sudo cp ${{ env.RUN_DIR }}/postprocess/croco_avg_t3.nc ${{ steps.archive_dir.outputs.value }}
            sudo cp ${{ env.RUN_DIR }}/output/croco_avg.nc ${{ steps.archive_dir.outputs.value }}
            sudo cp ${{ env.RUN_DIR }}/postprocess/croco_avg_frcst.nc ${{ steps.archive_dir.outputs.value }}
            sudo cp ${{ env.RUN_DIR }}/postprocess/croco_avg_temp_surf.gif ${{ steps.archive_dir.outputs.value }}
            sudo cp ${{ env.RUN_DIR }}/postprocess/croco_avg_temp_100m.gif ${{ steps.archive_dir.outputs.value }}
          fi
      - name: clean up
        run: |
          # the .nc files we previously copied files from elsewhere into the scratch directory are redundant
          # (it's useful to just dump them in scratch but are just copies so we don't need to keep them at all)
          sudo rm -f ${{ env.RUN_DIR }}/scratch/*.nc
 
